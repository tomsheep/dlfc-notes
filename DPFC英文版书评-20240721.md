## **一、背景**

Christopher Bishop本身就是一块闪亮的招牌，他2006年出版的《[Pattern Recognition and Machine Learning](https://book.douban.com/subject/2061116/)》（以下称PRML）长久以来就是机器学习领域的「圣经」之一，十多年前我就是读着这本书入行的。时隔20多年，他带来了《[Deep Learning: Foundations and Concepts](https://book.douban.com/subject/36666224/)》（以下称DLFC）这本新书，也没有让人失望。最近我用半个月左右的时间认真读了这本新书，体验非常愉悦。如果说在当下你需要一本「系统」地学习「现代」深度学习的教材，我认为这是不二之选，因为它做到了又「新」又「好」。

新在何处？好在何处？这首先要从机器学习领域的「变与不变」谈起。

### **1.1 「变」**

这十几年来，机器学习领域的发展可谓一日千里，在这个过程中，「深度学习」（深度神经网络）成为这10年最成功的范式（paradigm），成功到在很多场合可以和甚至「机器学习」这个上位词互相取代；而最近2年，以GPT为代表的「自回归的、基于Transformers的语言模型」（也就是当今炙手可热但又有些指代不明的热词「大语言模型」通常所指向的技术）又成为「深度学习」中最成功的「子范式」。

这从Bishop两部大作的命名上就可见一斑——2006年，Bishop 还在向读者揭示Pattern Recognition和Machine Learning两个领域的一体两面：前者来自工程学，尤其是电气工程和信号处理；后来源出计算机科学，尤其是人工智能。而在2024年，已经完全没有这个必要了，无论在学界还是业界，它们的理论、方法、应用都早已融合。更有甚者，「深度学习」这个范式已经如此重要，以至于在你需要写一本当代的「机器学习」教材的时候，你可以完全用「深度学习」的视角来阐释——这并不是说其他「机器学习」方法不再有价值，而是「深度学习」可以自成体系，而且很多传统机器学习中宝贵的思想和方法论，都可以融入到这个体系中来。

DLFC这本书很敏锐地捕捉到了这些年领域的变化，在选题上与时俱进，体现了最新的一些思潮和方法，包括不限于Transformers、LLM、GAN、Diffusion Models等方法都有所涵盖。单单做到这一点其实并不难，毕竟这是一本2023年底才付梓的书，你只要足够「新」就可以。但DLFC能做到把新的内容融入到一个清晰的体系中来，寓「变」于「不变」，让你看到来龙去脉，这就是Bishop的功力所在。

### **1.2 「不变」**

如果你读过PRML，读DLFC这本书时肯定会觉得似曾相识，它们的结构其实非常接近，很多内容也有重合，甚至可以说DLFC是PRML的「次时代重置版」——用这十多年来领域的新进展重塑了PRML。Bishop本人给了另一个说法，他把DLFC看做PRML的「伴随读物」（companion volume），因为它们互相之间并不能完全替代。这两个视角其实都是有道理的，读者不妨都保留。我在本文的后续部分也会展开介绍它们的关系。

最近我发现行业里有一种论调，很多人认为LLM这个范式会成为深度学习的绝对主流，就像深度学习之于机器学习那样，甚至于等同于「AI」。我个人对这个现象是持怀疑态度的，首先LLM虽然取得了很大的突破，但这个范式目前还远远没有达到大家预期中的统治力（我个人认为未来也不会，这个话题在本文不展开）。在媒体的推波助澜下，很多人——甚至很多有强专业背景的人——都认为「学AI只要从LLM学起就好了」，不要被旧时代的AI「污染」了。看到这样的论调我其实是很无语的（当然我并不坚持我是对的，或许我只是要被时代抛弃的遗老遗少），我的观点可以分为两个层次：

1. **LLM是一个很有趣也很有用的范式，但它存在严重的缺陷，目前也没有得到大量的市场检验，** 盛名之下其实难副 
2. 即便假以时日1被打脸，那么从实操的角度说，**「只从LLM学起」是很难把它学明白的，因为LLM目前还远远称不上「自成体系」**，它仍然需要被放在已有的体系中去学习，才能有深入的理解。

最近接触到一些过去没有太多机器学习领域经验的新人（我也不敢自居卖老，只能说是比我更新的新人），我常推荐他们去看一些科普性质的材料来获得「直觉」，其中李宏毅老师的课程是我比较推崇的，但我发现他们看完了之后，虽然确实获得了很好的「直觉」，但往往也有意想不到的副作用，比如有些人会说「2020年的论文都是上古时期的研究了，对今天价值不大」——这个论调当然是对李宏毅老师的「曲解」，前半句我估计他确实说过（我都能脑补出他机车的声线），但后半句绝对是学习者自己的错误引申。

在任何领域，**越偏向「策略」（policy）的东西，变得越快，而越偏向「原则」的东西，变得越慢。** 在机器学习这种卷到飞起的领域，前者可能变的非常快，常以周、月为频率更新，但这并不意味着后者也是如此，事实上一些核心的原则，也就是Bishop这本书标题里的「Foundations and Concepts」，它们的生命力往往以10年计，甚至更久。这也是2006年的PRML至今也不失为这个领域的经典教材的原因。

## **二、预备知识**

坦诚地说，Bishop的书对「新人」并不友好——PRML也是如此，10年前我刚开始读这本书时，也是几乎被劝退的——**他的书适合你在不同的学习阶段反复地读，直到你发现他讲的确实都是「基础」**，这才能读出精妙之处来。

和PRML近似，DLFC这本书首先需要具备一定程度的预备知识，说的笼统一点无非是「多元微积分」、「概率论」、「线性代数」这几个门类，但这个描述有点太泛泛了，因为这几个数学分支可深可浅，高阶内容和初级内容之间的难度能差出三条街来。我尝试按我的感受细化一下：

### 2.1 多元微积分

- 基础的高等数学
- 变分法（Calculus of Variations）：如果没有这部分知识，这本书里很多章节都涉及「证据下界」（ELBO）相关的方法和思路，就比较难看懂了
- 拉格朗日乘子法（Lagrange Multipliers）：凸优化的基础，也未必需要学的很深入，但需要理解基础的思想和概念

### 2.2 概率论

- 基础的概率与统计
- 多变量正态分布，太常用了
- 蒙特卡洛马尔科夫链（MCMC）：最好是学过「随机过程」，或者至少能理解MCMC的思想

### 2.3 线性代数

- 基础的线性代数
- 矩阵与向量的求导：不需要特别精通
- Jacobian/Hessian矩阵：能看懂公式、几何/物理意义

需要强调一下，**并不是说只有打好了所有这些基础知识之后才能开始看这本书，那样你可能永远开始不了，不妨先去读，然后按需去补充相应的基础知识**。但一定要去补，这是一本「教材」，「不求甚解」的结果就是你可能无法得到这本书的精髓，那就失去了看这类书的意义。

## **三、内容导读**

在这个章节，我会采取「DLFC作为PRML次时代重置版」的这个视角，来介绍这本书的结构：

### **3.1 轮廓**

DLFC和PRML可以说用了几乎同样的结构轮廓。「原则」和「策略」是交织的，在下面的介绍中，【原则】前缀代表这部分更偏重思想，【策略】前缀代表这部分更偏重具体算法，后者依赖前者，也是前者思想的一个具象落地。因此最推荐的仍然是线性循章节顺序阅读。

1、【原则】教学案例：

- Chap01用一个案例引出ML的历史和基础概念

2、【原则】概率论基础：

- Chap02 引出信息论、贝叶斯公式
- Chap03介绍常见的概率分布、概率密度估计的思想

3、【原则】单层网络：

- Chap04：回归问题，引出「**最大似然估计**」、「**偏差-方差权衡**」、决策论
- Chap05：分类问题，神经网络视角诠释逻辑回归；「**Canonical link functions**」

4、【原则】深度网络：

- Chap06：深度学习的基础思想，自动寻找Basis的视角；表示学习、迁移学习、对比学习思想的融入
- Chap07：梯度下降，batch/layer normalization，learning rate schedule
- Chap08：反向传播，自动微分
- Chap09：正则化，「**归纳偏置**」的思想；「**Double Descent**」现象；参数共享、残差链接、Dropout都安排在了这章，可以用正则化视角来看

5、【策略】CNN：

- Chap10：CNN的原理和应用，可以看作是Chap06-09思想的一个落地

6、【原则】Structured Distribution：

- Chap11: Graph Model 建模思想，贝叶斯网络基本推导方法；Sequence Models（刻画序列数据，比如语言模型）
- 引出「隐变量」方法，在后续Chap15-16还有深入讨论

7、【策略】Transformers：

- Chap12：Transformers、LLM原理和应用。把Transformers这章安排在了Chap11之后，我觉得作者是想表达可以把Transformers融入到「学习一个结构化的分布（Structured Distribution）」这个体系中去理解。（比如，学一个全连接的图）

8、【策略】GNN：

- Chap13：图模型的原理和应用

9、【原则】采样方法：

- Chap14：从「拒绝采样」到MCMC

10、【原则】隐变量：

- Chap15：离散隐变量；Kmeans算法引出EM算法
- Chap16：连续隐变量；用线性隐变量视角讲述PCA
- 这两章都涉及ELBO，需要「变分法」的数学基础
- 引出nonlinear隐变量，后续章节都是这个思想的应用

11、【策略】非线性隐变量在GenAI中的应用

- Chap17：GAN
- Chap18：归一化流
- Chap19：VAE
- Chap20：Diffusion Models

### **3.2 和PRML的差异**

1. PRML有大量的贝叶斯推断，但DLFC比较少，因为贝叶斯推断在深度学习领域应用并不主流
2. PRML的Chap05只用了一章讲神经网络，在DLFC里展开到了Chap06-09，体现了时代范式的变迁
3. PRML 的Chap06-07是Kernel functions，在DLFC里没有这个主题。深度学习中的神经网络，尤其是多层网络，通过其结构和训练过程，自动完成了类似于核函数的特征映射和非线性变换，因此不需要显式地设计核函数
4. PRML Chap10专门讲Approximate Inference，DLFC里没有，个人感觉加上可能也挺好。
5. PRML Chap14 专门讲Combining Models，DL中没有，因为彼时这在实践中有很广泛的直接应用，比如一些Tree-based的算法。在DLFC里没有专门讲述，但Boosting、Bagging的思想在正则化的主题下有所阐述

## **四、延伸阅读**

再次提醒，这是一本「教材」，而非「专著」，也就是说它的篇幅是受限的——没错，虽然它长达600多页，但如果展开写，恐怕篇幅要膨胀3倍都不止——其中重在阐述「思想」（为什么），让读者获得「宏观的视野」（有什么），而更具体的细节以及「怎么做」则需要读者更深入的自我钻研。

### **4.1 推荐的学习路径**

我个人比较推荐的学习路径，大致是「直觉」->「思想」->「专项」这三个层次反复循环

- 直觉：类似李宏毅老师的课程
- 思想：PRML/DLFC是这阶段的良好读物
- 专项：所需的垂类方向的论文、实践项目

「思想」阶段是否有类似的读物？

- Goodfellow、Bengio 2017年的《[Deep Learning](https://book.douban.com/subject/26883982/)》
- Murphy的系列[“Probabilistic machine learning”: a book series by Kevin Murphy](https://probml.github.io/pml-book/)（这个系列我其实没看过，篇幅太长了，但口碑很好，有机会读的话再分享心得）

## **五、一些花絮**

这本书还有个第二作者Hugh Bishop，是Christopher Bishop的儿子，想必是PRML致辞页里全家福中那两个小男孩之一。

![](https://picx.zhimg.com/v2-ece8d6065d2af1e0e15dce4adf25d947_1440w.jpg)